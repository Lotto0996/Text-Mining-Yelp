{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tip_file = 'yelp_academic_dataset_tip.json'\n",
    "tip_records = [json.loads(line) for line in open(tip_file)]\n",
    "len(tip_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_file = 'yelp_academic_dataset_checkin.json'\n",
    "checkin_records = [json.loads(line) for line in open(checkin_file)]\n",
    "len(checkin_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4153150"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "review_file = 'yelp_academic_dataset_review.json'\n",
    "reviews = [json.loads(line) for line in open(review_file)]\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "business_file = 'yelp_academic_dataset_business.json'\n",
    "business_records = [json.loads(line) for line in open(business_file)]\n",
    "len(business_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Business records:',len(business_records))\n",
    "print ('Tip records:',len(tip_records))\n",
    "print ('Checkin records:',len(checkin_records))\n",
    "print ('Reviews:',len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Analyze Business and Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities=[]\n",
    "for l in business_records:\n",
    "    cities.append(l['city'])\n",
    "city_list=list(set(cities))\n",
    "print ('no of cities',len(city_list))\n",
    "\n",
    "states=[]\n",
    "for l in business_records:\n",
    "    states.append(l['state'])\n",
    "state_list=list(set(states))\n",
    "print ('no of states',len(state_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "bus_city=defaultdict(list)\n",
    "bus_state=defaultdict(list)\n",
    "for b in business_records:\n",
    "    bus_city[b['city']].append(1)\n",
    "    bus_state[b['state']].append(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "w=[]\n",
    "for c in city_list:\n",
    "    w.append((c,sum(bus_city[c])))\n",
    "    x+=sum(bus_city[c])\n",
    "w=sorted(w, key=lambda x: -x[1])\n",
    "print (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww=[]\n",
    "x=0\n",
    "print ('state, #reviews')\n",
    "for c in state_list:\n",
    "    ww.append((c,sum(bus_state[c])))\n",
    "    x+=sum(bus_state[c])\n",
    "ww=sorted(ww, key=lambda x: -x[1])\n",
    "print (ww,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "busID_state=defaultdict(list)\n",
    "busID_city=defaultdict(list)\n",
    "for b in business_records:\n",
    "    busID_state[b['business_id']]=b['state']\n",
    "    busID_city[b['business_id']]=b['city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_city=defaultdict(list)\n",
    "reviews_state=defaultdict(list)\n",
    "for r in reviews:\n",
    "    reviews_city[busID_city[r['business_id']]].append(1)\n",
    "    reviews_state[busID_state[r['business_id']]].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "w=[]\n",
    "print ('state, #reviews')\n",
    "for c in state_list:\n",
    "    w.append((c,sum(reviews_state[c])))\n",
    "    x+=sum(reviews_state[c])\n",
    "w=sorted(w, key=lambda x: -x[1])\n",
    "print (w,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filt data, standard deviation of ratings, overlapping rating histogram after normalization, n-grams,\n",
    "#reviews per cities histogram, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "ww=[]\n",
    "print ('city, #reviews')\n",
    "filtered_city_list=['Montréal','Edinburgh','Pittsburgh','Stuttgart']\n",
    "for c in filtered_city_list:\n",
    "    ww.append((c,sum(reviews_city[c])))\n",
    "    x+=sum(reviews_city[c])\n",
    "ww=sorted(ww, key=lambda x: -x[1])\n",
    "print (ww,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_rating=defaultdict(list)\n",
    "s=[]\n",
    "for r in reviews:\n",
    "    rev_rating[r['stars']].append(1)\n",
    "    s.append(r['stars'])\n",
    "print ('stars, #reviews')\n",
    "\n",
    "for x in list(set(s)):\n",
    "    print (x,sum(rev_rating[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww=[]\n",
    "for x in list(set(s)):\n",
    "    ww.append((x,sum(rev_rating[x])))\n",
    "print (ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_year=defaultdict(list)\n",
    "rev_rating_year=defaultdict(list)\n",
    "rev_rating_month=defaultdict(list)\n",
    "rev_rating_state=defaultdict(list)\n",
    "rev_rating_city=defaultdict(list)\n",
    "y=[]\n",
    "m=[]\n",
    "for r in reviews[:10000]:\n",
    "    rev_year[r['date'].split('-')[0]].append(1)\n",
    "    rev_rating_year[r['date'].split('-')[0]].append(r['stars'])\n",
    "    rev_rating_month[r['date'].split('-')[1]].append(r['stars'])\n",
    "    rev_rating_state[busID_state[r['business_id']]].append(r['stars'])\n",
    "    rev_rating_city[busID_city[r['business_id']]].append(r['stars'])\n",
    "    y.append(r['date'].split('-')[0])\n",
    "    m.append(r['date'].split('-')[1])\n",
    "temp=0\n",
    "print ('year, #reviews, avg. stars')\n",
    "for x in sorted(list(set(y))):\n",
    "    q=sum(rev_year[x]) #total reviews in year 'x'\n",
    "    print (x,q,float(sum(rev_rating_year[x]))/q)\n",
    "    temp+=sum(rev_year[x])\n",
    "print ('month, #reviews, avg. stars')\n",
    "for x in sorted(list(set(m))):\n",
    "    q=len(rev_rating_month[x]) #total reviews in month 'x'\n",
    "    print (x,q,float(sum(rev_rating_month[x]))/q)\n",
    "print ('city, avg. stars')\n",
    "for st in filtered_city_list:\n",
    "    print (st,sum(rev_rating_city[st])/len(rev_rating_city[st]))\n",
    "# for ci in city_list:\n",
    "#     print (ci,sum(rev_rating_city[ci])/len(rev_rating_city[ci]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ('year, #reviews, avg. stars')\n",
    "w1=[]\n",
    "w2=[]\n",
    "w3=[]\n",
    "w4=[]\n",
    "w5=[]\n",
    "for x in sorted(list(set(y))):\n",
    "    q=sum(rev_year[x]) #total reviews in year 'x'\n",
    "    w1.append((x,q))\n",
    "    w2.append((x,float(sum(rev_rating_year[x]))/q, np.std(rev_rating_year[x])))\n",
    "#     temp+=sum(rev_year[x])\n",
    "# print ('month, #reviews, avg. stars')\n",
    "for x in sorted(list(set(m))):\n",
    "    q=len(rev_rating_month[x]) #total reviews in month 'x'\n",
    "    w3.append((x,q))\n",
    "    w4.append((x,float(sum(rev_rating_month[x]))/q, np.std(rev_rating_month[x])))\n",
    "# print ('city, avg. stars')\n",
    "for st in filtered_city_list:\n",
    "    w5.append((st,sum(rev_rating_city[st])/len(rev_rating_city[st])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww=[]\n",
    "for x in list(set(s)):\n",
    "    ww.append((x,sum(rev_rating[x])))\n",
    "print (ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "filtered_city_list=['Montréal','Edinburgh','Pittsburgh','Stuttgart']\n",
    "reviews_filtered=[]\n",
    "for r in reviews:\n",
    "    if busID_city[r['business_id']] in filtered_city_list:\n",
    "        reviews_filtered.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frev_year=defaultdict(list)\n",
    "frev_rating_year=defaultdict(list)\n",
    "frev_rating_month=defaultdict(list)\n",
    "frev_rating_state=defaultdict(list)\n",
    "frev_rating_city=defaultdict(list)\n",
    "for r in reviews_filtered:\n",
    "    frev_year[r['date'].split('-')[0]].append(1)\n",
    "    frev_rating_year[r['date'].split('-')[0]].append(r['stars'])\n",
    "    frev_rating_month[r['date'].split('-')[1]].append(r['stars'])\n",
    "    frev_rating_state[busID_state[r['business_id']]].append(r['stars'])\n",
    "    frev_rating_city[busID_city[r['business_id']]].append(r['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww1=[]\n",
    "ww2=[]\n",
    "ww3=[]\n",
    "ww4=[]\n",
    "ww5=[]\n",
    "for x in sorted(list(set(y))):\n",
    "    q=sum(frev_year[x]) #total reviews in year 'x'\n",
    "    ww1.append((x,q))\n",
    "    ww2.append((x,float(sum(frev_rating_year[x]))/q, np.std(frev_rating_year[x])))\n",
    "#     temp+=sum(rev_year[x])\n",
    "# print ('month, #reviews, avg. stars')\n",
    "for x in sorted(list(set(m))):\n",
    "    q=len(frev_rating_month[x]) #total reviews in month 'x'\n",
    "    ww3.append((x,q))\n",
    "    ww4.append((x,float(sum(frev_rating_month[x]))/q, np.std(frev_rating_month[x])))\n",
    "# print ('city, avg. stars')\n",
    "for st in filtered_city_list:\n",
    "    ww5.append((st,sum(frev_rating_city[st])/len(frev_rating_city[st]),np.std(frev_rating_city[st])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frev_rating=defaultdict(list)\n",
    "fs=[]\n",
    "for r in reviews_filtered:\n",
    "    frev_rating[r['stars']].append(1)\n",
    "    fs.append(r['stars'])\n",
    "print ('stars, #reviews')\n",
    "ww=[]\n",
    "for x in list(set(fs)):\n",
    "    ww.append((x,sum(frev_rating[x])))\n",
    "print (ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "\n",
    "bb=defaultdict(list)\n",
    "s=[]\n",
    "# for r in reviews:\n",
    "#     bb[r['city']].append(1)\n",
    "    \n",
    "for l in business_records:\n",
    "    if l['city'] in filtered_city_list:\n",
    "        bb[l['city']].append(1)\n",
    "        x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=[]\n",
    "for c in filtered_city_list:\n",
    "    aa.append((c,sum(bb[c])))\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(2, 2), preprocessor=None,\n",
       "        stop_words=['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing'...', 'wollen', 'wollte', 'während', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen', 'über'],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "import string\n",
    "from itertools import groupby\n",
    "from stop_words import get_stop_words\n",
    "from collections import Counter\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "stop = get_stop_words('english')\n",
    "english_cities=['Montréal']#, 'Montréal', 'Edinburgh', 'Stuttgart']\n",
    "T=CountVectorizer(lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
    "                  ngram_range=(word_len, word_len), stop_words=stop1+stop2)\n",
    "\n",
    "city = 'Montréal'\n",
    "all_reviews = []\n",
    "for r in reviews_filtered:\n",
    "    if busID_city[r['business_id']] == city:\n",
    "        all_reviews.append(r['text'])\n",
    "T.fit(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "1\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "mostcounts =[]\n",
    "for season in seasons:\n",
    "# for city in english_cities:\n",
    "    for s in [1,5]:\n",
    "        print (s)\n",
    "        temp=[]\n",
    "        for r in reviews_filtered:\n",
    "            if busID_city[r['business_id']] == city:\n",
    "                if int(r['date'].split('-')[1]) in season:\n",
    "                    if r['stars']==s:\n",
    "                        temp.append(r['text'])\n",
    "        for word_len in [1,2]:\n",
    "#             ng = []\n",
    "            print (word_len)\n",
    "#             for text in temp:\n",
    "#                 sentence = ''.join([c for c in text.lower() if not c in punctuation])\n",
    "#                 sentence = ' '.join([w for w in sentence.split() if not w in stop])\n",
    "#             T=CountVectorizer(lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
    "#             ngram_range=(word_len, word_len), stop_words=stop1+stop2)\n",
    "#                 tokenize = nltk.word_tokenize(sentence)\n",
    "#                 sixgrams=ngrams(tokenize,word_len)\n",
    "#                 for grams in list(sixgrams):\n",
    "#                     ng.append(' '.join(grams))\n",
    "#             counts = Counter(ng)\n",
    "            counts = T.transform(temp)\n",
    "            freq = np.sum(counts, axis=0)\n",
    "            vocab = list(T.vocabulary_)\n",
    "            tup = list(zip(vocab,freq.tolist()[0]))\n",
    "#             mostcounts = counts.most_common(100)\n",
    "            alice_mask = np.array(Image.open(\"y.jpg\"))\n",
    "            stopwords = set(STOPWORDS)\n",
    "            # stopwords.add(\"said\")\n",
    "            wc = WordCloud(background_color=\"white\", max_words=100, mask=alice_mask,\n",
    "                           stopwords=stopwords)\n",
    "            wc.fit_words(tup)\n",
    "#             wc.fit_words(mostcounts)\n",
    "            # store to file\n",
    "            wc.to_file(season[0]+'rating'+str(s)+'_'+str(word_len)+\"gram.png\")\n",
    "    print (len(mostcounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Read the whole text.\n",
    "text = open('TXT.txt').read()\n",
    "\n",
    "alice_mask = np.array(Image.open(\"y.jpg\"))\n",
    "stopwords = set(STOPWORDS)\n",
    "# stopwords.add(\"said\")\n",
    "wc = WordCloud(background_color=\"white\", max_words=100, mask=alice_mask,\n",
    "               stopwords=stopwords)\n",
    "wc.fit_words(counts.most_common(100))\n",
    "# store to file\n",
    "wc.to_file(\"dada.png\")\n",
    "# show\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for r in reviews:\n",
    "    l.append(len(r['text'].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "data = l\n",
    "bins = np.linspace(math.ceil(min(data)), \n",
    "                   math.floor(max(data)),\n",
    "                   100) # fixed number of bins\n",
    "\n",
    "plt.xlim([min(data)-5, max(data)+5])\n",
    "\n",
    "plt.hist(data, bins=bins, alpha=0.5, color='b')\n",
    "plt.title('Complete data: Review length vs. # reviews')\n",
    "plt.xlabel('Reviews length')\n",
    "plt.ylabel('# of reviews')\n",
    "plt.savefig('8.1.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_city_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx=[]\n",
    "for city in filtered_city_list:\n",
    "    one, two, three, four, five =0,0,0,0,0\n",
    "    for r in reviews_filtered:\n",
    "        if busID_city[r['business_id']]==city:\n",
    "            if r['stars']==1:\n",
    "                one+=1\n",
    "            elif r['stars']==2:\n",
    "                two+=1\n",
    "            elif r['stars']==3:\n",
    "                three+=1\n",
    "            elif r['stars']==4:\n",
    "                four+=1\n",
    "            else:\n",
    "                five+=1\n",
    "    xxx.append((city,[one,two,three,four,five]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw_data = {'City': ['Montréal', 'Edinburgh', 'Pittsburgh', 'Stuttgart'],\n",
    "        '1': [6859,1953,14734,2146],\n",
    "        '2': [7195,2883,13132,2333],\n",
    "        '3': [14039,8378,19941,3751],\n",
    "        '4': [32416,17786,40962,7266],\n",
    "        '5': [36001,14482,54349,9140]\n",
    "           }\n",
    "df = pd.DataFrame(raw_data, columns = ['City', '1','2','3','4','5'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a single subplot\n",
    "f, ax = plt.subplots(1, figsize=(10,5))\n",
    "\n",
    "# Set bar width at 1\n",
    "bar_width = 1\n",
    "\n",
    "# positions of the left bar-boundaries\n",
    "bar_l = [i for i in range(len(df['1']))]\n",
    "\n",
    "# positions of the x-axis ticks (center of the bars as bar labels)\n",
    "tick_pos = [i+(bar_width/2) for i in bar_l]\n",
    "\n",
    "# Create the total score for each participant\n",
    "totals = [i+j+k+l+m for i,j,k,l,m in zip(df['1'], df['2'], df['3'],df['4'],df['5'])]\n",
    "\n",
    "rel_1 = [i / j * 100 for  i,j in zip(df['1'], totals)]\n",
    "rel_2 = [i / j * 100 for  i,j in zip(df['2'], totals)]\n",
    "rel_3 = [i / j * 100 for  i,j in zip(df['3'], totals)]\n",
    "rel_4 = [i / j * 100 for  i,j in zip(df['4'], totals)]\n",
    "rel_5 = [i / j * 100 for  i,j in zip(df['5'], totals)]\n",
    "\n",
    "# Create a bar chart in position bar_1\n",
    "ax.bar(bar_l,\n",
    "       # using pre_rel data\n",
    "       rel_1,\n",
    "       # labeled\n",
    "       label='Pre Score',\n",
    "       # with alpha\n",
    "       alpha=0.9,\n",
    "       # with color\n",
    "       color='black',\n",
    "       # with bar width\n",
    "       width=bar_width,\n",
    "       # with border color\n",
    "       edgecolor='white'\n",
    "       )\n",
    "\n",
    "# Create a bar chart in position bar_1\n",
    "ax.bar(bar_l,\n",
    "       # using mid_rel data\n",
    "       rel_2,\n",
    "       # with pre_rel\n",
    "       bottom=rel_1,\n",
    "       # labeled\n",
    "       label='Mid Score',\n",
    "       # with alpha\n",
    "       alpha=0.9,\n",
    "       # with color\n",
    "       color='#3C5F5A',\n",
    "       # with bar width\n",
    "       width=bar_width,\n",
    "       # with border color\n",
    "       edgecolor='white'\n",
    "       )\n",
    "\n",
    "# Create a bar chart in position bar_1\n",
    "ax.bar(bar_l,\n",
    "       # using post_rel data\n",
    "       rel_3,\n",
    "       # with pre_rel and mid_rel on bottom\n",
    "       bottom=[i+j for i,j in zip(rel_1, rel_2)],\n",
    "       # labeled\n",
    "       label='Post Score',\n",
    "       # with alpha\n",
    "       alpha=0.9,\n",
    "       # with color\n",
    "       color='blue',\n",
    "       # with bar width\n",
    "       width=bar_width,\n",
    "       # with border color\n",
    "       edgecolor='white'\n",
    "       )\n",
    "ax.bar(bar_l,\n",
    "       # using post_rel data\n",
    "       rel_4,\n",
    "       # with pre_rel and mid_rel on bottom\n",
    "       bottom=[i+j+k for i,j,k in zip(rel_1, rel_2,rel_3)],\n",
    "       # labeled\n",
    "       label='Post Score 1',\n",
    "       # with alpha\n",
    "       alpha=0.9,\n",
    "       # with color\n",
    "       color='#219AD8',\n",
    "       # with bar width\n",
    "       width=bar_width,\n",
    "       # with border color\n",
    "       edgecolor='white'\n",
    "       )\n",
    "\n",
    "ax.bar(bar_l,\n",
    "       # using post_rel data\n",
    "       rel_5,\n",
    "       # with pre_rel and mid_rel on bottom\n",
    "       bottom=[i+j+l+m for i,j,l,m in zip(rel_1, rel_2,rel_3,rel_4)],\n",
    "       # labeled\n",
    "       label='Post Score 2',\n",
    "       # with alpha\n",
    "       alpha=0.9,\n",
    "       # with color\n",
    "       color='green',\n",
    "       # with bar width\n",
    "       width=bar_width,\n",
    "       # with border color\n",
    "       edgecolor='white'\n",
    "       )\n",
    "\n",
    "\n",
    "# Set the ticks to be first names\n",
    "plt.xticks(tick_pos, df['City'])\n",
    "ax.set_ylabel(\"Percentage of reviews\")\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "# Let the borders of the graphic\n",
    "plt.xlim([min(tick_pos)-bar_width, max(tick_pos)])\n",
    "plt.ylim(0, 100)\n",
    "# rotate axis labels\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=0, horizontalalignment='right')\n",
    "plt.title('Rating distribution: 1 star at bottom (black) to 5 star at top (green))')\n",
    "plt.savefig('9.1.png',dpi=300)\n",
    "# shot plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edinburgh\n",
      "Stuttgart\n",
      "Montréal\n",
      "Pittsburgh\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "stop2 = get_stop_words('german')\n",
    "stop1 = get_stop_words('english')\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from textstat.textstat import textstat\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.special import expit\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "# fall=['fall',10,11,12]\n",
    "# spring=['spring',4,5,6]\n",
    "# winter=['winter',1,2,3]\n",
    "# summer=['summer',7,8,9]\n",
    "# seasons = [winter, spring, summer, fall]\n",
    "\n",
    "spring=['2010s',2011,2012,2013,2014,2015,2016,2017]\n",
    "winter=['2000s',2003,2004,2005,2006,2007,2008,2009,2010]\n",
    "# seasons = [winter, spring]\n",
    "seasons = ['Edinburgh', 'Stuttgart','Montréal', 'Pittsburgh']\n",
    "# english_cities=['Montréal','Edinburgh', 'Pittsburgh']\n",
    "# nonenglish_cities=['Stuttgart']\n",
    "# for city in english_cities:\n",
    "#     print (city)\n",
    "#     for season in seasons:\n",
    "english_reviews=[]\n",
    "english_stars=[]\n",
    "# nonenglish_reviews=[]\n",
    "# nonenglish_stars=[]\n",
    "for r in reviews_filtered:\n",
    "#     if busID_city[r['business_id']] in english_cities:\n",
    "    english_reviews.append(r)\n",
    "    english_stars.append(r['stars'])\n",
    "#     elif busID_city[r['business_id']] in nonenglish_cities:\n",
    "#         nonenglish_reviews.append(r)\n",
    "#         nonenglish_stars.append(r['stars'])\n",
    "\n",
    "english=[]\n",
    "english_stars=[]\n",
    "english_test=[]\n",
    "english_stars_test=[]\n",
    "\n",
    "for season in seasons:\n",
    "    print (season)\n",
    "    all_1stars_text = []\n",
    "    all_2stars_text = []\n",
    "    all_3stars_text = []\n",
    "    all_4stars_text = []\n",
    "    all_5stars_text = []\n",
    "    for r in english_reviews:\n",
    "#         if int(r['date'].split('-')[0]) in season:\n",
    "        if busID_city[r['business_id']] == season:\n",
    "            if r['stars']==1:\n",
    "                all_1stars_text.append(r['text'])\n",
    "            elif r['stars']==2:\n",
    "                all_2stars_text.append(r['text'])\n",
    "            elif r['stars']==3:\n",
    "                all_3stars_text.append(r['text'])\n",
    "            elif r['stars']==4:\n",
    "                all_4stars_text.append(r['text'])\n",
    "            else:\n",
    "                all_5stars_text.append(r['text'])\n",
    "\n",
    "    all_1stars_labels = [1.0]*len(all_1stars_text)\n",
    "    all_2stars_labels = [2.0]*len(all_2stars_text)\n",
    "    all_3stars_labels = [3.0]*len(all_3stars_text)\n",
    "    all_4stars_labels = [4.0]*len(all_4stars_text)\n",
    "    all_5stars_labels = [5.0]*len(all_5stars_text)\n",
    "\n",
    "    # gall_1stars_text = []\n",
    "    # gall_2stars_text = []\n",
    "    # gall_3stars_text = []\n",
    "    # gall_4stars_text = []\n",
    "    # gall_5stars_text = []\n",
    "\n",
    "    # for r in nonenglish_reviews:\n",
    "    #     if r['stars']==1:\n",
    "    #         gall_1stars_text.append(r['text'])\n",
    "    #     elif r['stars']==2:\n",
    "    #         gall_2stars_text.append(r['text'])\n",
    "    #     elif r['stars']==3:\n",
    "    #         gall_3stars_text.append(r['text'])\n",
    "    #     elif r['stars']==4:\n",
    "    #         gall_4stars_text.append(r['text'])\n",
    "    #     else:\n",
    "    #         gall_5stars_text.append(r['text'])\n",
    "\n",
    "    # gall_1stars_labels = [1.0]*len(gall_1stars_text)\n",
    "    # gall_2stars_labels = [2.0]*len(gall_2stars_text)\n",
    "    # gall_3stars_labels = [3.0]*len(gall_3stars_text)\n",
    "    # gall_4stars_labels = [4.0]*len(gall_4stars_text)\n",
    "    # gall_5stars_labels = [5.0]*len(gall_5stars_text)\n",
    "\n",
    "    all_1stars_text_train, all_1stars_text_test, all_1stars_labels_train, all_1stars_labels_test = train_test_split(all_1stars_text, all_1stars_labels, test_size=0.20)\n",
    "    all_2stars_text_train, all_2stars_text_test, all_2stars_labels_train, all_2stars_labels_test = train_test_split(all_2stars_text, all_2stars_labels, test_size=0.20)\n",
    "    all_3stars_text_train, all_3stars_text_test, all_3stars_labels_train, all_3stars_labels_test = train_test_split(all_3stars_text, all_3stars_labels, test_size=0.20)\n",
    "    all_4stars_text_train, all_4stars_text_test, all_4stars_labels_train, all_4stars_labels_test = train_test_split(all_4stars_text, all_4stars_labels, test_size=0.20)\n",
    "    all_5stars_text_train, all_5stars_text_test, all_5stars_labels_train, all_5stars_labels_test = train_test_split(all_5stars_text, all_5stars_labels, test_size=0.20)\n",
    "\n",
    "    # gall_1stars_text_train, gall_1stars_text_test, gall_1stars_labels_train, gall_1stars_labels_test = train_test_split(gall_1stars_text, gall_1stars_labels, test_size=0.20)\n",
    "    # gall_2stars_text_train, gall_2stars_text_test, gall_2stars_labels_train, gall_2stars_labels_test = train_test_split(gall_2stars_text, gall_2stars_labels, test_size=0.20)\n",
    "    # gall_3stars_text_train, gall_3stars_text_test, gall_3stars_labels_train, gall_3stars_labels_test = train_test_split(gall_3stars_text, gall_3stars_labels, test_size=0.20)\n",
    "    # gall_4stars_text_train, gall_4stars_text_test, gall_4stars_labels_train, gall_4stars_labels_test = train_test_split(gall_4stars_text, gall_4stars_labels, test_size=0.20)\n",
    "    # gall_5stars_text_train, gall_5stars_text_test, gall_5stars_labels_train, gall_5stars_labels_test = train_test_split(gall_5stars_text, gall_5stars_labels, test_size=0.20)\n",
    "\n",
    "#         english=all_1stars_text_train+all_2stars_text_train+all_3stars_text_train+all_4stars_text_train+all_5stars_text_train\n",
    "#         # german = gall_1stars_text_train+gall_2stars_text_train+gall_3stars_text_train+gall_4stars_text_train+gall_5stars_text_train\n",
    "\n",
    "#         english_test=all_1stars_text_test+all_2stars_text_test+all_3stars_text_test+all_4stars_text_test+all_5stars_text_test\n",
    "#         # german_test = gall_1stars_text_test+gall_2stars_text_test+gall_3stars_text_test+gall_4stars_text_test+gall_5stars_text_test\n",
    "\n",
    "#         english_stars=np.array(all_1stars_labels_train+all_2stars_labels_train+all_3stars_labels_train+all_4stars_labels_train+all_5stars_labels_train)\n",
    "#         # german_stars = np.array(gall_1stars_labels_train+gall_2stars_labels_train+gall_3stars_labels_train+gall_4stars_labels_train+gall_5stars_labels_train)\n",
    "\n",
    "#         english_stars_test=np.array(all_1stars_labels_test+all_2stars_labels_test+all_3stars_labels_test+all_4stars_labels_test+all_5stars_labels_test)\n",
    "#         # german_stars_test = np.array(gall_1stars_labels_test+gall_2stars_labels_test+gall_3stars_labels_test+gall_4stars_labels_test+gall_5stars_labels_test)\n",
    "\n",
    "\n",
    "    english.append(all_1stars_text_train+all_2stars_text_train+all_3stars_text_train+all_4stars_text_train+all_5stars_text_train)\n",
    "    # german = gall_1stars_text_train+gall_2stars_text_train+gall_3stars_text_train+gall_4stars_text_train+gall_5stars_text_train\n",
    "\n",
    "    english_test.append(all_1stars_text_test+all_2stars_text_test+all_3stars_text_test+all_4stars_text_test+all_5stars_text_test)\n",
    "    # german_test = gall_1stars_text_test+gall_2stars_text_test+gall_3stars_text_test+gall_4stars_text_test+gall_5stars_text_test\n",
    "\n",
    "    english_stars.append(np.array(all_1stars_labels_train+all_2stars_labels_train+all_3stars_labels_train+all_4stars_labels_train+all_5stars_labels_train))\n",
    "    # german_stars = np.array(gall_1stars_labels_train+gall_2stars_labels_train+gall_3stars_labels_train+gall_4stars_labels_train+gall_5stars_labels_train)\n",
    "\n",
    "    english_stars_test.append(np.array(all_1stars_labels_test+all_2stars_labels_test+all_3stars_labels_test+all_4stars_labels_test+all_5stars_labels_test))\n",
    "    # german_stars_test = np.array(gall_1stars_labels_test+gall_2stars_labels_test+gall_3stars_labels_test+gall_4stars_labels_test+gall_5stars_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.008\n",
      "1.448\n",
      "1.202\n",
      "1.378\n"
     ]
    }
   ],
   "source": [
    "N = [(2,2)]\n",
    "for i in range(4):\n",
    "#         mse_german = []\n",
    "    for n1,n2 in N:\n",
    "        english_vectorizer=TfidfVectorizer(lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
    "                ngram_range=(n1, n2), stop_words=stop1+stop2)\n",
    "    #     german_vectorizer=CountVectorizer(lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
    "    #             ngram_range=(n1, n2), stop_words=stop1+stop2)\n",
    "        X_english_train = english_vectorizer.fit_transform(english[i]).toarray()#[0]+english[1]+english[2]+english[3]).toarray()\n",
    "    #     X_english_test = english_vectorizer.transform(english_test).toarray()\n",
    "    #     X_german_train = german_vectorizer.fit_transform(german).toarray()\n",
    "    #     X_german_test = german_vectorizer.transform(german_test).toarray()\n",
    "    #     reg = RandomForestRegressor(n_estimators=15, criterion='mse', max_depth=20)#GradientBoostingRegressor(n_estimators=100, criterion='mse')\n",
    "    #     reg.fit(X_english_train,english_stars)\n",
    "    #     english_stars_pred = reg.predict(X_english_test)\n",
    "#         reg = linear_model.LinearRegression()\n",
    "        reg=GradientBoostingRegressor(n_estimators=200,min_impurity_split=1e-3)\n",
    "#         reg=LinearSVC()\n",
    "        reg.fit (X_english_train, list(english_stars[i]))#+list(english_stars[1])+list(english_stars[2])+list(english_stars[3]))\n",
    "\n",
    "        X_english_test = english_vectorizer.transform(english_test[i]).toarray()\n",
    "        english_stars_pred = reg.predict((X_english_test))#+list(english_stars_test[1])+list(english_stars_test[2])+list(english_stars_test[3]))\n",
    "\n",
    "    #     reg = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=20)#GradientBoostingRegressor(n_estimators=100, criterion='mse')\n",
    "    # #     reg = linear_model.LinearRegression()\n",
    "    #     reg.fit(X_german_train, german_stars)\n",
    "    #     german_stars_pred = reg.predict(X_german_test)\n",
    "        mm=mean_squared_error(english_stars_pred, english_stars_test[i])\n",
    "        print (round(mm,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010s 0.747\n",
      "2011 0.729\n"
     ]
    }
   ],
   "source": [
    "N = [(1,1)]#, (1,2), (2,2)]#(3,3),(1,2),(2,3),(1,3)]\n",
    "\n",
    "mse_english = []\n",
    "for i in range(2):\n",
    "#         mse_german = []\n",
    "    for n1,n2 in N:\n",
    "        english_vectorizer=TfidfVectorizer(lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
    "                ngram_range=(n1, n2), stop_words=stop1+stop2)\n",
    "    #     german_vectorizer=CountVectorizer(lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
    "    #             ngram_range=(n1, n2), stop_words=stop1+stop2)\n",
    "        X_english_train = english_vectorizer.fit_transform(english[i]).toarray()#[0]+english[1]+english[2]+english[3]).toarray()\n",
    "    #     X_english_test = english_vectorizer.transform(english_test).toarray()\n",
    "    #     X_german_train = german_vectorizer.fit_transform(german).toarray()\n",
    "    #     X_german_test = german_vectorizer.transform(german_test).toarray()\n",
    "    #     reg = RandomForestRegressor(n_estimators=15, criterion='mse', max_depth=20)#GradientBoostingRegressor(n_estimators=100, criterion='mse')\n",
    "    #     reg.fit(X_english_train,english_stars)\n",
    "    #     english_stars_pred = reg.predict(X_english_test)\n",
    "        reg = linear_model.LinearRegression()\n",
    "    #     reg=GradientBoostingRegressor(n_estimators=1000)\n",
    "    #         reg=LinearSVC()\n",
    "        reg.fit (X_english_train, english_stars[i])#list(english_stars[0])+list(english_stars[1])+list(english_stars[2])+list(english_stars[3]))\n",
    "\n",
    "        X_english_test = english_vectorizer.transform(english_test[i]).toarray()\n",
    "        english_stars_pred = reg.predict(X_english_test)\n",
    "\n",
    "    #     reg = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=20)#GradientBoostingRegressor(n_estimators=100, criterion='mse')\n",
    "    # #     reg = linear_model.LinearRegression()\n",
    "    #     reg.fit(X_german_train, german_stars)\n",
    "    #     german_stars_pred = reg.predict(X_german_test)\n",
    "        mm=mean_squared_error(english_stars_pred, english_stars_test[i])\n",
    "        print (season[i],round(mm,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "N = [(1,1)]#(3,3),(1,2),(2,3),(1,3)]\n",
    "\n",
    "mse_english = []\n",
    "mse_german = []\n",
    "for n1,n2 in N:\n",
    "    english_vectorizer=TfidfVectorizer(lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
    "            ngram_range=(n1, n2), stop_words=stop1)\n",
    "#     german_vectorizer=CountVectorizer(lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
    "#             ngram_range=(n1, n2), stop_words=stop1+stop2)\n",
    "    X_english_train = english_vectorizer.fit_transform(english).toarray()\n",
    "    X_english_test = english_vectorizer.transform(english_test).toarray()\n",
    "#     X_german_train = german_vectorizer.fit_transform(german).toarray()\n",
    "#     X_german_test = german_vectorizer.transform(german_test).toarray()\n",
    "#     reg = RandomForestRegressor(n_estimators=15, criterion='mse', max_depth=20)#GradientBoostingRegressor(n_estimators=100, criterion='mse')\n",
    "#     reg.fit(X_english_train,english_stars)\n",
    "#     english_stars_pred = reg.predict(X_english_test)\n",
    "#     reg = linear_model.LinearRegression()\n",
    "    reg=AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=50)#GradientBoostingRegressor(n_estimators=50)\n",
    "    reg.fit (X_english_train, english_stars)\n",
    "    english_stars_pred = reg.predict(X_english_test)\n",
    "    \n",
    "#     reg = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=20)#GradientBoostingRegressor(n_estimators=100, criterion='mse')\n",
    "# #     reg = linear_model.LinearRegression()\n",
    "#     reg.fit(X_german_train, german_stars)\n",
    "#     german_stars_pred = reg.predict(X_german_test)\n",
    "    mm=mean_squared_error(english_stars_pred, english_stars_test)\n",
    "    mse_english.append(mm) \n",
    "    print (n1,n2,mm)\n",
    "#     mse_german.append(mean_squared_error(german_stars_pred, german_stars_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=20)#GradientBoostingRegressor(n_estimators=100, criterion='mse')\n",
    "# #     reg = linear_model.LinearRegression()\n",
    "# reg.fit(X_german_train, german_stars)\n",
    "# german_stars_pred = reg.predict(X_german_test)\n",
    "# #     mse_english.append(mean_squared_error(english_stars_pred, english_stars_test)) \n",
    "# mse_german.append(mean_squared_error(german_stars_pred, german_stars_test))\n",
    "# mse_german\n",
    "\n",
    "reg = RandomForestRegressor(n_estimators=15, criterion='mse', max_depth=20)#GradientBoostingRegressor(n_estimators=100, criterion='mse')\n",
    "reg.fit(X_english_train,english_stars)\n",
    "english_stars_pred = reg.predict(X_english_test)\n",
    "mse_english.append(mean_squared_error(english_stars_pred, english_stars_test)) \n",
    "mse_english\n",
    "\n",
    "# np.array([N,str(mse_english),str(mse_german)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vowpalwabbit import pyvw\n",
    "# vw = pyvw.vw(quiet=True)\n",
    "# from sklearn import datasets\n",
    "# from vowpalwabbit.sklearn_vw import VWClassifier\n",
    "\n",
    "#  # generate some data\n",
    "# model = VWClassifier()\n",
    "# model.fit(X_english_train,english_stars)\n",
    "# # model.score(X_train, y_train)\n",
    "# english_stars_pred = model.predict(X_english_test)\n",
    "# mean_squared_error(english_stars_pred, english_stars_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business_id': 'oiTq668GwyXo5hMHjdW0kA',\n",
       " 'cool': 0,\n",
       " 'date': '2014-12-04',\n",
       " 'funny': 0,\n",
       " 'review_id': 'gC0pCs7oywnKmesZwwl_JA',\n",
       " 'stars': 3,\n",
       " 'text': \"Your typical burger in a pub.  The great part about Stack'd is that you get to build your own burger and put whatever toppings you want on your burger, from guacamole to a fried egg to whatever vegetables your heart desires.  A little overcooked for my taste and the meat is 'eh', but the toppings and cheap prices are enough to satisfy your late night burger cravings (and the 3-star rating).\",\n",
       " 'type': 'review',\n",
       " 'useful': 0,\n",
       " 'user_id': 'w1ubQ8xkCfA3f4gaQUr_3Q'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[]\n",
    "for r in reviews_filtered:\n",
    "    if busID_city[r['business_id']] in english_cities:\n",
    "        if r['useful'] or r['cool'] or r['funny']:\n",
    "            temp.append([r['useful'],r['cool'],r['funny']])\n",
    "temp=np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48680, 3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components = 3)\n",
    "pca.fit(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 23.9737778 ,   1.10051455,   0.72412819])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for b in business_records:\n",
    "    if b['categories']:\n",
    "        if 'Restaurants' in b['categories']:\n",
    "            x.append(b['business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214261"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=0\n",
    "x=set(x)\n",
    "for r in reviews_filtered:\n",
    "    if r['business_id'] in x:\n",
    "            t+=1\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': '7804 Rea Rd',\n",
       " 'attributes': ['Alcohol: full_bar',\n",
       "  \"Ambience: {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': True, 'upscale': False, 'casual': False}\",\n",
       "  \"BestNights: {'monday': True, 'tuesday': True, 'friday': False, 'wednesday': True, 'thursday': False, 'sunday': False, 'saturday': False}\",\n",
       "  'BikeParking: True',\n",
       "  'BusinessAcceptsCreditCards: True',\n",
       "  \"BusinessParking: {'garage': False, 'street': False, 'validated': False, 'lot': False, 'valet': False}\",\n",
       "  'Caters: False',\n",
       "  \"DietaryRestrictions: {'dairy-free': False, 'gluten-free': True, 'vegan': False, 'kosher': False, 'halal': False, 'soy-free': False, 'vegetarian': False}\",\n",
       "  'GoodForKids: True',\n",
       "  \"GoodForMeal: {'dessert': False, 'latenight': False, 'lunch': False, 'dinner': True, 'breakfast': False, 'brunch': False}\",\n",
       "  'HasTV: True',\n",
       "  \"Music: {'dj': False, 'background_music': False, 'no_music': False, 'karaoke': False, 'live': False, 'video': False, 'jukebox': False}\",\n",
       "  'NoiseLevel: average',\n",
       "  'OutdoorSeating: True',\n",
       "  'RestaurantsAttire: casual',\n",
       "  'RestaurantsDelivery: False',\n",
       "  'RestaurantsGoodForGroups: True',\n",
       "  'RestaurantsPriceRange2: 2',\n",
       "  'RestaurantsReservations: True',\n",
       "  'RestaurantsTableService: True',\n",
       "  'RestaurantsTakeOut: True',\n",
       "  'Smoking: no',\n",
       "  'WiFi: free',\n",
       "  'GoodForDancing: False',\n",
       "  'HappyHour: True'],\n",
       " 'business_id': 'FrfzI901uS2bU2kgUkqaVw',\n",
       " 'categories': ['Seafood',\n",
       "  'Restaurants',\n",
       "  'Bars',\n",
       "  'Cocktail Bars',\n",
       "  'Nightlife'],\n",
       " 'city': 'Charlotte',\n",
       " 'hours': ['Monday 11:0-23:0',\n",
       "  'Tuesday 11:0-23:0',\n",
       "  'Wednesday 11:0-23:0',\n",
       "  'Thursday 11:0-23:0',\n",
       "  'Friday 11:0-0:0',\n",
       "  'Saturday 11:0-0:0',\n",
       "  'Sunday 11:0-23:0'],\n",
       " 'is_open': 1,\n",
       " 'latitude': 35.060865737,\n",
       " 'longitude': -80.8144658479,\n",
       " 'name': 'Pearlz Oyster Bar',\n",
       " 'neighborhood': '',\n",
       " 'postal_code': '28277',\n",
       " 'review_count': 70,\n",
       " 'stars': 3.5,\n",
       " 'state': 'NC',\n",
       " 'type': 'business'}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest[660]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest=[]\n",
    "for b in business_records:\n",
    "    if b['business_id'] in x:\n",
    "        rest.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_Seafood=[]\n",
    "for r in rest:\n",
    "    if 'Seafood' in r['categories']:\n",
    "        r_Seafood.append(r)\n",
    "# len(list(set(ff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Restaurants', 48485),\n",
       " ('Food', 8575),\n",
       " ('Nightlife', 6334),\n",
       " ('Bars', 6067),\n",
       " ('American (Traditional)', 5312),\n",
       " ('Fast Food', 5250),\n",
       " ('Pizza', 5229),\n",
       " ('Sandwiches', 5220),\n",
       " ('Italian', 4118),\n",
       " ('Burgers', 3868),\n",
       " ('Mexican', 3673),\n",
       " ('American (New)', 3621),\n",
       " ('Chinese', 3611),\n",
       " ('Breakfast & Brunch', 3556),\n",
       " ('Cafes', 2574),\n",
       " ('Coffee & Tea', 2075),\n",
       " ('Japanese', 2054),\n",
       " ('Chicken Wings', 1718),\n",
       " ('Seafood', 1697),\n",
       " ('Sushi Bars', 1691),\n",
       " ('Event Planning & Services', 1597),\n",
       " ('Delis', 1470),\n",
       " ('Asian Fusion', 1409),\n",
       " ('Salad', 1404),\n",
       " ('Mediterranean', 1339),\n",
       " ('Sports Bars', 1318),\n",
       " ('Canadian (New)', 1286),\n",
       " ('Barbeque', 1279),\n",
       " ('Steakhouses', 1228),\n",
       " ('Indian', 1223),\n",
       " ('Thai', 1222),\n",
       " ('Specialty Food', 1207),\n",
       " ('Pubs', 1173),\n",
       " ('Bakeries', 1078),\n",
       " ('Diners', 1046),\n",
       " ('Caterers', 1041),\n",
       " ('Desserts', 931),\n",
       " ('French', 903),\n",
       " ('Greek', 894),\n",
       " ('Middle Eastern', 889),\n",
       " ('Vietnamese', 855),\n",
       " ('Vegetarian', 819),\n",
       " ('Wine & Spirits', 718),\n",
       " ('Beer', 718),\n",
       " ('Wine Bars', 712),\n",
       " ('Buffets', 710),\n",
       " ('Arts & Entertainment', 650),\n",
       " ('Korean', 639),\n",
       " ('Lounges', 622),\n",
       " ('Tex-Mex', 609),\n",
       " ('Hot Dogs', 551),\n",
       " ('Ice Cream & Frozen Yogurt', 548),\n",
       " ('Comfort Food', 534),\n",
       " ('Gluten-Free', 532),\n",
       " ('Cocktail Bars', 526),\n",
       " ('Ethnic Food', 524),\n",
       " ('Soup', 514),\n",
       " ('Vegan', 506),\n",
       " ('Food Delivery Services', 472),\n",
       " ('German', 469),\n",
       " ('Gastropubs', 466),\n",
       " ('Caribbean', 465),\n",
       " ('Juice Bars & Smoothies', 464),\n",
       " ('Latin American', 418),\n",
       " ('Southern', 356),\n",
       " ('Grocery', 346),\n",
       " ('Food Trucks', 341),\n",
       " ('Bagels', 333),\n",
       " ('Halal', 332),\n",
       " ('Pakistani', 326),\n",
       " ('Tapas/Small Plates', 325),\n",
       " ('British', 304),\n",
       " ('Hotels & Travel', 297),\n",
       " ('Shopping', 296),\n",
       " ('Venues & Event Spaces', 291),\n",
       " ('Tapas Bars', 286),\n",
       " ('Music Venues', 279),\n",
       " ('Fish & Chips', 270),\n",
       " ('Hotels', 245),\n",
       " ('Food Stands', 235),\n",
       " ('Dim Sum', 220),\n",
       " ('Imported Food', 215),\n",
       " ('Soul Food', 212),\n",
       " ('Hawaiian', 207),\n",
       " ('Cajun/Creole', 206),\n",
       " ('Portuguese', 202),\n",
       " ('Modern European', 197),\n",
       " ('Dive Bars', 194),\n",
       " ('Breweries', 191),\n",
       " ('Creperies', 190),\n",
       " ('Spanish', 177),\n",
       " ('Active Life', 177),\n",
       " ('Noodles', 176),\n",
       " ('Swabian', 174),\n",
       " ('Irish', 168),\n",
       " ('Filipino', 161),\n",
       " ('Turkish', 160),\n",
       " ('Street Vendors', 158),\n",
       " ('Chicken Shop', 155),\n",
       " ('Local Flavor', 153),\n",
       " ('Cheesesteaks', 152),\n",
       " ('Taiwanese', 152),\n",
       " ('Lebanese', 147),\n",
       " ('Tea Rooms', 140),\n",
       " ('Karaoke', 140),\n",
       " ('Dance Clubs', 138),\n",
       " ('Persian/Iranian', 138),\n",
       " ('Donuts', 128),\n",
       " ('Brasseries', 126),\n",
       " ('African', 121),\n",
       " ('Ramen', 121),\n",
       " ('Beer Bar', 118),\n",
       " ('Food Court', 111),\n",
       " ('Meat Shops', 108),\n",
       " ('Party & Event Planning', 106),\n",
       " ('Bistros', 101),\n",
       " ('Casinos', 96),\n",
       " ('International', 94),\n",
       " ('Kosher', 91),\n",
       " ('Falafel', 85),\n",
       " ('Peruvian', 84),\n",
       " ('Hookah Bars', 82),\n",
       " ('Poutineries', 81),\n",
       " ('Health Markets', 80),\n",
       " ('Afghan', 80),\n",
       " ('Convenience Stores', 76),\n",
       " ('Delicatessen', 74),\n",
       " ('Beer Garden', 74),\n",
       " ('Brazilian', 73),\n",
       " ('Scottish', 71),\n",
       " ('Bubble Tea', 71),\n",
       " ('Szechuan', 70),\n",
       " ('Ethiopian', 66),\n",
       " ('Cantonese', 64),\n",
       " ('Hot Pot', 62),\n",
       " ('Polish', 61),\n",
       " ('Cuban', 60),\n",
       " ('Internet Cafes', 59),\n",
       " ('Jazz & Blues', 58),\n",
       " ('Malaysian', 58),\n",
       " ('Seafood Markets', 58),\n",
       " ('Butcher', 56),\n",
       " ('Do-It-Yourself Food', 55),\n",
       " ('Beauty & Spas', 55),\n",
       " ('Moroccan', 54),\n",
       " ('Arcades', 53),\n",
       " ('Cheese Shops', 52),\n",
       " ('Golf', 51),\n",
       " ('Live/Raw Food', 51),\n",
       " ('Arabian', 51),\n",
       " ('Local Services', 50),\n",
       " ('Waffles', 48),\n",
       " ('Kebab', 47),\n",
       " ('Pool Halls', 47),\n",
       " ('Salvadoran', 45),\n",
       " ('Russian', 45),\n",
       " ('Himalayan/Nepalese', 44),\n",
       " ('Organic Stores', 44),\n",
       " ('Belgian', 43),\n",
       " ('Patisserie/Cake Shop', 43),\n",
       " ('Fondue', 43),\n",
       " ('Fruits & Veggies', 43),\n",
       " ('Mongolian', 41),\n",
       " ('Home & Garden', 40),\n",
       " ('Automotive', 40),\n",
       " ('Shaved Ice', 37),\n",
       " ('Farmers Market', 37),\n",
       " ('Chocolatiers & Shops', 36),\n",
       " ('Colombian', 36),\n",
       " ('Beer Gardens', 35),\n",
       " ('Health & Medical', 35),\n",
       " ('Irish Pub', 34),\n",
       " ('Gelato', 34),\n",
       " ('Tacos', 34),\n",
       " ('Shopping Centers', 34),\n",
       " ('Performing Arts', 33),\n",
       " ('Pasta Shops', 32),\n",
       " ('Flowers & Gifts', 32),\n",
       " ('Cambodian', 31),\n",
       " ('Argentine', 31),\n",
       " ('Home Services', 30),\n",
       " ('Fashion', 30),\n",
       " ('Education', 30),\n",
       " ('Wineries', 29),\n",
       " ('Art Galleries', 29),\n",
       " ('Cupcakes', 29),\n",
       " ('Bowling', 28),\n",
       " ('Smokehouse', 28),\n",
       " ('Basque', 27),\n",
       " ('Poke', 27),\n",
       " ('Hungarian', 26),\n",
       " ('Pretzels', 26),\n",
       " ('Gas & Service Stations', 26),\n",
       " ('Wedding Planning', 25),\n",
       " ('Hair Salons', 25),\n",
       " ('Cafeteria', 24),\n",
       " ('Festivals', 24),\n",
       " ('Mags', 23),\n",
       " ('Music & Video', 23),\n",
       " ('Books', 23)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.Ridge(7)\n",
    "reg.fit (X_english_train, english_stars)\n",
    "english_stars_pred = reg.predict(X_english_test)\n",
    "print (mean_squared_error(english_stars_pred, english_stars_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(english_stars), mean_squared_error(english_stars_test,[np.mean(english_stars)]*english_stars_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edinburgh\n",
      "Stuttgart\n",
      "Montréal\n",
      "Pittsburgh\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [114492, 77206]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-de927d828197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;31m#     reg=AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=50)#GradientBoostingRegressor(n_estimators=50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_english_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_stars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0menglish_stars_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_english_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sakaray/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 512\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sakaray/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sakaray/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [114492, 77206]"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "stop2 = get_stop_words('german')\n",
    "stop1 = get_stop_words('english')\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from textstat.textstat import textstat\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.special import expit\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "# fall=['fall',10,11,12]\n",
    "# spring=['spring',4,5,6]\n",
    "# winter=['winter',1,2,3]\n",
    "# summer=['summer',7,8,9]\n",
    "# seasons = [winter, spring, summer, fall]\n",
    "\n",
    "# spring=['2010s',2011,2012,2013,2014,2015,2016,2017]\n",
    "# winter=['2000s',2003,2004,2005,2006,2007,2008,2009,2010]\n",
    "# seasons = [winter, spring]\n",
    "seasons = ['Edinburgh', 'Stuttgart','Montréal', 'Pittsburgh']\n",
    "# english_cities=['Montréal','Edinburgh', 'Pittsburgh']\n",
    "# nonenglish_cities=['Stuttgart']\n",
    "# for city in english_cities:\n",
    "#     print (city)\n",
    "#     for season in seasons:\n",
    "english_reviews=[]\n",
    "english_stars=[]\n",
    "# nonenglish_reviews=[]\n",
    "# nonenglish_stars=[]\n",
    "for r in reviews_filtered:\n",
    "#     if busID_city[r['business_id']] in english_cities:\n",
    "    english_reviews.append(r)\n",
    "    english_stars.append(r['stars'])\n",
    "#     elif busID_city[r['business_id']] in nonenglish_cities:\n",
    "#         nonenglish_reviews.append(r)\n",
    "#         nonenglish_stars.append(r['stars'])\n",
    "\n",
    "english=[]\n",
    "english_stars=[]\n",
    "english_test=[]\n",
    "english_stars_test=[]\n",
    "\n",
    "for season in seasons:\n",
    "    print (season)\n",
    "    all_1stars_text = []\n",
    "    all_2stars_text = []\n",
    "    all_3stars_text = []\n",
    "    all_4stars_text = []\n",
    "    all_5stars_text = []\n",
    "    for r in english_reviews:\n",
    "#         if int(r['date'].split('-')[0]) in season:\n",
    "        if busID_city[r['business_id']] == season:\n",
    "            if r['stars']==1:\n",
    "                all_1stars_text.append(r['text'])\n",
    "            elif r['stars']==2:\n",
    "                all_2stars_text.append(r['text'])\n",
    "            elif r['stars']==3:\n",
    "                all_3stars_text.append(r['text'])\n",
    "            elif r['stars']==4:\n",
    "                all_4stars_text.append(r['text'])\n",
    "            else:\n",
    "                all_5stars_text.append(r['text'])\n",
    "\n",
    "    all_1stars_labels = [1.0]*len(all_1stars_text)\n",
    "    all_2stars_labels = [2.0]*len(all_2stars_text)\n",
    "    all_3stars_labels = [3.0]*len(all_3stars_text)\n",
    "    all_4stars_labels = [4.0]*len(all_4stars_text)\n",
    "    all_5stars_labels = [5.0]*len(all_5stars_text)\n",
    "\n",
    "    # gall_1stars_text = []\n",
    "    # gall_2stars_text = []\n",
    "    # gall_3stars_text = []\n",
    "    # gall_4stars_text = []\n",
    "    # gall_5stars_text = []\n",
    "\n",
    "    # for r in nonenglish_reviews:\n",
    "    #     if r['stars']==1:\n",
    "    #         gall_1stars_text.append(r['text'])\n",
    "    #     elif r['stars']==2:\n",
    "    #         gall_2stars_text.append(r['text'])\n",
    "    #     elif r['stars']==3:\n",
    "    #         gall_3stars_text.append(r['text'])\n",
    "    #     elif r['stars']==4:\n",
    "    #         gall_4stars_text.append(r['text'])\n",
    "    #     else:\n",
    "    #         gall_5stars_text.append(r['text'])\n",
    "\n",
    "    # gall_1stars_labels = [1.0]*len(gall_1stars_text)\n",
    "    # gall_2stars_labels = [2.0]*len(gall_2stars_text)\n",
    "    # gall_3stars_labels = [3.0]*len(gall_3stars_text)\n",
    "    # gall_4stars_labels = [4.0]*len(gall_4stars_text)\n",
    "    # gall_5stars_labels = [5.0]*len(gall_5stars_text)\n",
    "\n",
    "    all_1stars_text_train, all_1stars_text_test, all_1stars_labels_train, all_1stars_labels_test = train_test_split(all_1stars_text, all_1stars_labels, test_size=0.20)\n",
    "    all_2stars_text_train, all_2stars_text_test, all_2stars_labels_train, all_2stars_labels_test = train_test_split(all_2stars_text, all_2stars_labels, test_size=0.20)\n",
    "    all_3stars_text_train, all_3stars_text_test, all_3stars_labels_train, all_3stars_labels_test = train_test_split(all_3stars_text, all_3stars_labels, test_size=0.20)\n",
    "    all_4stars_text_train, all_4stars_text_test, all_4stars_labels_train, all_4stars_labels_test = train_test_split(all_4stars_text, all_4stars_labels, test_size=0.20)\n",
    "    all_5stars_text_train, all_5stars_text_test, all_5stars_labels_train, all_5stars_labels_test = train_test_split(all_5stars_text, all_5stars_labels, test_size=0.20)\n",
    "\n",
    "    # gall_1stars_text_train, gall_1stars_text_test, gall_1stars_labels_train, gall_1stars_labels_test = train_test_split(gall_1stars_text, gall_1stars_labels, test_size=0.20)\n",
    "    # gall_2stars_text_train, gall_2stars_text_test, gall_2stars_labels_train, gall_2stars_labels_test = train_test_split(gall_2stars_text, gall_2stars_labels, test_size=0.20)\n",
    "    # gall_3stars_text_train, gall_3stars_text_test, gall_3stars_labels_train, gall_3stars_labels_test = train_test_split(gall_3stars_text, gall_3stars_labels, test_size=0.20)\n",
    "    # gall_4stars_text_train, gall_4stars_text_test, gall_4stars_labels_train, gall_4stars_labels_test = train_test_split(gall_4stars_text, gall_4stars_labels, test_size=0.20)\n",
    "    # gall_5stars_text_train, gall_5stars_text_test, gall_5stars_labels_train, gall_5stars_labels_test = train_test_split(gall_5stars_text, gall_5stars_labels, test_size=0.20)\n",
    "\n",
    "#         english=all_1stars_text_train+all_2stars_text_train+all_3stars_text_train+all_4stars_text_train+all_5stars_text_train\n",
    "#         # german = gall_1stars_text_train+gall_2stars_text_train+gall_3stars_text_train+gall_4stars_text_train+gall_5stars_text_train\n",
    "\n",
    "#         english_test=all_1stars_text_test+all_2stars_text_test+all_3stars_text_test+all_4stars_text_test+all_5stars_text_test\n",
    "#         # german_test = gall_1stars_text_test+gall_2stars_text_test+gall_3stars_text_test+gall_4stars_text_test+gall_5stars_text_test\n",
    "\n",
    "#         english_stars=np.array(all_1stars_labels_train+all_2stars_labels_train+all_3stars_labels_train+all_4stars_labels_train+all_5stars_labels_train)\n",
    "#         # german_stars = np.array(gall_1stars_labels_train+gall_2stars_labels_train+gall_3stars_labels_train+gall_4stars_labels_train+gall_5stars_labels_train)\n",
    "\n",
    "#         english_stars_test=np.array(all_1stars_labels_test+all_2stars_labels_test+all_3stars_labels_test+all_4stars_labels_test+all_5stars_labels_test)\n",
    "#         # german_stars_test = np.array(gall_1stars_labels_test+gall_2stars_labels_test+gall_3stars_labels_test+gall_4stars_labels_test+gall_5stars_labels_test)\n",
    "\n",
    "\n",
    "    english.append(all_1stars_text_train+all_2stars_text_train+all_3stars_text_train+all_4stars_text_train+all_5stars_text_train)\n",
    "    # german = gall_1stars_text_train+gall_2stars_text_train+gall_3stars_text_train+gall_4stars_text_train+gall_5stars_text_train\n",
    "\n",
    "    english_test.append(all_1stars_text_test+all_2stars_text_test+all_3stars_text_test+all_4stars_text_test+all_5stars_text_test)\n",
    "    # german_test = gall_1stars_text_test+gall_2stars_text_test+gall_3stars_text_test+gall_4stars_text_test+gall_5stars_text_test\n",
    "\n",
    "    english_stars.append(np.array(all_1stars_labels_train+all_2stars_labels_train+all_3stars_labels_train+all_4stars_labels_train+all_5stars_labels_train))\n",
    "    # german_stars = np.array(gall_1stars_labels_train+gall_2stars_labels_train+gall_3stars_labels_train+gall_4stars_labels_train+gall_5stars_labels_train)\n",
    "\n",
    "    english_stars_test.append(np.array(all_1stars_labels_test+all_2stars_labels_test+all_3stars_labels_test+all_4stars_labels_test+all_5stars_labels_test))\n",
    "    # german_stars_test = np.array(gall_1stars_labels_test+gall_2stars_labels_test+gall_3stars_labels_test+gall_4stars_labels_test+gall_5stars_labels_test)\n",
    "\n",
    "# transformer = TfidfTransformer(smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0.769151927973\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = [(1,1)]#(3,3),(1,2),(2,3),(1,3)]\n",
    "\n",
    "mse_english = []\n",
    "mse_german = []\n",
    "for n1,n2 in N:\n",
    "    english_vectorizer=TfidfVectorizer(lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
    "            ngram_range=(n1, n2), stop_words=stop1)\n",
    "#     german_vectorizer=CountVectorizer(lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
    "#             ngram_range=(n1, n2), stop_words=stop1+stop2)\n",
    "    X_english_train = english_vectorizer.fit_transform(english[3]).toarray()\n",
    "    X_english_test = english_vectorizer.transform(english_test[2]).toarray()\n",
    "#     X_german_train = german_vectorizer.fit_transform(german).toarray()\n",
    "#     X_german_test = german_vectorizer.transform(german_test).toarray()\n",
    "#     reg = RandomForestRegressor(n_estimators=15, criterion='mse', max_depth=20)#GradientBoostingRegressor(n_estimators=100, criterion='mse')\n",
    "#     reg.fit(X_english_train,english_stars)\n",
    "#     english_stars_pred = reg.predict(X_english_test)\n",
    "    reg = linear_model.LinearRegression()\n",
    "#     reg=AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=50)#GradientBoostingRegressor(n_estimators=50)\n",
    "    reg.fit (X_english_train, english_stars[3])\n",
    "    english_stars_pred = reg.predict(X_english_test)\n",
    "    \n",
    "#     reg = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=20)#GradientBoostingRegressor(n_estimators=100, criterion='mse')\n",
    "# #     reg = linear_model.LinearRegression()\n",
    "#     reg.fit(X_german_train, german_stars)\n",
    "#     german_stars_pred = reg.predict(X_german_test)\n",
    "    mm=mean_squared_error(english_stars_pred, english_stars_test[2])\n",
    "    mse_english.append(mm) \n",
    "    print (n1,n2,mm)\n",
    "#     mse_german.append(mean_squared_error(german_stars_pred, german_stars_test))Seen by Bhanu Vikas at 19:36"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
